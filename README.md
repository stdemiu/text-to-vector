Этот код строит и обучает улучшенную нейронную сеть для многоклассовой классификации текстов, используя слои Dropout для регуляризации и раннее завершение для предотвращения избыточного обучения. 

## Импорт библиотек
```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
```

- **import torch** — Импортируем PyTorch, основную библиотеку для работы с нейронными сетями, которая позволяет легко работать с тензорами, моделями и обучением.
- **import torch.nn as nn** — Импортируем модуль для создания нейронных сетей nn из PyTorch. В нем содержатся основные компоненты сети, такие как линейные слои, функции активации и функции потерь.
- **import torch.optim as optim** — Импортируем optim из PyTorch, который содержит алгоритмы оптимизации, такие как Adam, SGD и другие. Они используются для обновления весов модели.
- **from torch.utils.data import DataLoader, TensorDataset** — Импортируем классы DataLoader и TensorDataset.
- **TensorDataset** позволяет объединять данные и метки в один объект для удобной работы с PyTorch.
- **DataLoader** обеспечивает загрузку данных в небольших пакетах (батчах) и может перемешивать данные для лучшего обучения.
- **import pandas as pd** — Импортируем pandas, удобную библиотеку для работы с таблицами данных, особенно для загрузки данных из CSV.
- **from sklearn.feature_extraction.text import TfidfVectorizer** — Импортируем TfidfVectorizer из sklearn. Он преобразует текст в числовое представление на основе TF-IDF, взвешивая слова по их значимости в документе и в корпусе.

## Шаг 1. Векторизация данных и загрузка меток
### Загрузка данных
```python
data = pd.read_csv('sherlock_holmes_text_with_labels.csv')
texts = data['Text']
labels = pd.get_dummies(data[['Label1', 'Label2']])
```

- **data = pd.read_csv(...)** — загружаем данные из файла sherlock_holmes_text_with_labels.csv в формате CSV, используя функцию read_csv из pandas.

Этот файл должен содержать колонку с текстами (Text) и колонки с метками (Label1, Label2).
Загруженные данные будут сохранены в data как DataFrame, представляющий таблицу, где каждая строка — отдельный текстовый пример.

- **texts = data['Text']** — извлекаем столбец с текстами из таблицы и сохраняем его в переменной texts.
Теперь texts содержит только текстовые данные из файла.
-**labels = pd.get_dummies(data[['Label1', 'Label2']])** — используем get_dummies, чтобы создать бинарные представления меток.
get_dummies преобразует каждое уникальное значение в колонках Label1 и Label2 в отдельную колонку со значениями 0 и 1. Этот формат удобен для многоклассовой классификации.
Переменная labels теперь содержит бинарные метки для каждого текста.

### Векторизация текста с помощью TF-IDF
```python
vectorizer = TfidfVectorizer()
text_vectors = vectorizer.fit_transform(texts)
X = text_vectors.toarray()
y = labels.values
```

- **vectorizer = TfidfVectorizer()** — создаем объект TfidfVectorizer, который будет преобразовывать текст в числовое представление.
- **TF-IDF (Term Frequency - Inverse Document Frequency)** взвешивает слова в зависимости от их частоты в тексте и в наборе данных. Часто встречающиеся в одном тексте, но редкие в других тексты слова получают больший вес.
- **text_vectors = vectorizer.fit_transform(texts)** — обучаем TfidfVectorizer на текстах и преобразуем каждый текст в числовой вектор.
- **fit_transform** одновременно "обучает" векторизатор и преобразует тексты в числовое представление.
- **text_vectors** — разреженная матрица, где каждая строка — это текст, а столбцы представляют уникальные слова, взвешенные по TF-IDF.
- **X = text_vectors.toarray()** — преобразуем разреженную матрицу в плотный массив numpy для удобства работы с PyTorch.
X теперь содержит массивы чисел для каждого текста.
- **y = labels.values** — извлекаем значения меток в виде массива numpy.
y — массив, где каждая строка соответствует меткам для одного текста, а колонки представляют бинарные метки для каждого класса.

### Преобразование данных в тензоры
```python
X_tensor = torch.tensor(X, dtype=torch.float32)
y_tensor = torch.tensor(y, dtype=torch.float32)
```

- **X_tensor = torch.tensor(X, dtype=torch.float32)** — конвертируем массив X в тензор PyTorch, указав тип float32.
X_tensor теперь готов к использованию в нейронной сети PyTorch.
- **y_tensor = torch.tensor(y, dtype=torch.float32)** — аналогично, конвертируем метки в тензор.
y_tensor теперь содержит тензоры меток и также готов для работы с моделью.

### Создание датасета и загрузчика данных
```python
dataset = TensorDataset(X_tensor, y_tensor)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)
```

- **dataset = TensorDataset(X_tensor, y_tensor)** — создаем TensorDataset, который объединяет X_tensor и y_tensor.
TensorDataset позволяет работать с парой данных и меток как с единым объектом. Он будет использоваться для подачи данных в модель.
- **dataloader = DataLoader(dataset, batch_size=32, shuffle=True)** — создаем DataLoader для итеративной загрузки данных.
- **batch_size=32** означает, что данные будут загружаться в пакетах по 32 примера.
- **shuffle=True** перемешивает данные в начале каждой эпохи, что помогает улучшить обобщающую способность модели.

## Шаг 2. Улучшенная модель с дополнительными слоями и Dropout
Определение класса модели
```python
class ImprovedNN(nn.Module):
    def __init__(self, input_dim, hidden_dim1, hidden_dim2, hidden_dim3, output_dim):
        super(ImprovedNN, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim1)
        self.relu1 = nn.ReLU()
        self.dropout1 = nn.Dropout(p=0.3)
        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)
        self.relu2 = nn.ReLU()
        self.dropout2 = nn.Dropout(p=0.3)
        self.fc3 = nn.Linear(hidden_dim2, hidden_dim3)
        self.relu3 = nn.ReLU()
        self.fc4 = nn.Linear(hidden_dim3, output_dim)
```
- class ImprovedNN(nn.Module): — создаем класс ImprovedNN, наследующий nn.Module, который является базовым классом PyTorch для нейронных сетей.
#### __init__ — конструктор класса:
- **super(ImprovedNN, self).__init__()** — инициализирует базовый класс nn.Module.
- **self.fc1 = nn.Linear(input_dim, hidden_dim1)** — первый линейный слой, преобразующий входную размерность input_dim в hidden_dim1.
- **self.relu1 = nn.ReLU()** — функция активации ReLU, которая добавляет нелинейность, устанавливая отрицательные значения на 0.
- **self.dropout1 = nn.Dropout(p=0.3)** — слой Dropout, который случайным образом обнуляет нейроны с вероятностью 30% для регуляризации.
- **self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)** — второй линейный слой, который принимает вход из hidden_dim1 и выводит hidden_dim2.
- **self.relu2 и self.dropout2** — аналогичные функции активации и Dropout для второго слоя.
- **self.fc3 и self.relu3** — третий слой и функция активации.
- **self.fc4 = nn.Linear(hidden_dim3, output_dim)** — выходной слой, который преобразует данные в размерность, равную количеству классов.

### Функция forward для прохождения данных
```python
def forward(self, x):
    x = self.dropout1(self.relu1(self.fc1(x)))
    x = self.dropout2(self.relu2(self.fc2(x)))
    x = self.relu3(self.fc3(x))
    x = self.fc4(x)
    return x
```
- **def forward(self, x):** — метод forward определяет порядок, в котором данные проходят через слои сети.
- **self.fc1(x)** — первый линейный слой.
- **self.relu1(...)** — функция активации ReLU.
- **self.dropout1(...)** — слой Dropout.

Процесс повторяется для второго и третьего слоев.

- **self.fc4(x)** — выходной слой, преобразующий вектора в размерность, равную числу классов.
- **return x** — возвращаем результат, проходящий через слои.

## Шаг 3. Обучение модели с ранним завершением и увеличенным числом эпох
Определение функции потерь и оптимизатора
```python
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model.parameters(), lr=0.0005)
```
- **criterion = nn.BCEWithLogitsLoss()** — функция потерь, которая объединяет сигмоид и бинарную кросс-энтропию, оптимально подходя для многоклассовой классификации.
- **optimizer = optim.Adam(...)** — оптимизатор Adam, который адаптирует темпы обучения для каждого параметра.

### Процесс обучения с ранним завершением
```python
num_epochs = 200
patience = 10
best_loss = float('inf')
counter = 0

for epoch in range(num_epochs):
    epoch_loss = 0
    for inputs, labels in dataloader:
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        epoch_loss += loss.item()
    
    epoch_loss /= len(dataloader)
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')
    
    if epoch_loss < best_loss:
        best_loss = epoch_loss
        counter = 0
        torch.save(model.state_dict(), "model.pth")
    else:
        counter += 1
        if counter >= patience:
            print("Раннее завершение обучения")
            break
```
Раннее завершение:
- Если потери не уменьшаются 10 эпох подряд, обучение завершится.
- torch.save(...) сохраняет модель с наименьшими потерями.

- **num_epochs = 200** — устанавливаем максимальное количество эпох для обучения. Модель будет обучаться максимум 200 раз на всех данных, но обучение может завершиться раньше, если потери не улучшаются в течение определенного количества эпох (настройка раннего завершения).
- **patience = 10** — устанавливаем количество эпох терпения для раннего завершения. Если потери (loss) не уменьшаются на протяжении 10 последовательных эпох, обучение прервется. Это помогает избежать лишних вычислений, если модель больше не улучшает свои результаты.
- **best_loss = float('inf')** — начальное значение для отслеживания минимальных потерь. Изначально оно задано как бесконечность (float('inf')), чтобы любая фактическая потеря в первую эпоху была меньше и могла обновить best_loss.
- **counter = 0** — счетчик для отслеживания количества эпох, в которых потери не улучшаются. Если потери не уменьшаются в текущей эпохе, счетчик увеличивается на 1. Когда счетчик достигает значения patience, обучение прекращается.
- **for epoch in range(num_epochs):** — цикл по эпохам, от 1 до num_epochs. Каждая эпоха проходит через все данные один раз.
- **epoch_loss = 0** — инициализация переменной epoch_loss, которая будет суммировать потери по всем пакетам (batch) в текущей эпохе. В конце каждой эпохи среднее значение epoch_loss будет рассчитано и использовано для отслеживания улучшений.
- **for inputs, labels in dataloader:** — внутренняя петля по пакетам данных (batch) в dataloader. Здесь inputs — это пакет (батч) входных данных, а labels — метки для этих данных.
- **outputs = model(inputs)** — пропускаем пакет inputs через модель, получая предсказанные выходные значения outputs.
- **loss = criterion(outputs, labels)** — рассчитываем потери (ошибку) для текущего пакета, используя функцию потерь criterion, которая сравнивает предсказанные значения outputs и истинные метки labels. Это значение loss указывает, насколько предсказания отклоняются от истинных меток.
- **optimizer.zero_grad()** — сбрасываем (обнуляем) градиенты для всех параметров модели, чтобы избежать накопления градиентов из предыдущих итераций.
- **loss.backward()** — вычисляем градиенты функции потерь по всем параметрам модели. Эти градиенты указывают, в каком направлении и на сколько нужно изменить параметры для минимизации потерь.
- **optimizer.step()** — обновляем параметры модели с использованием оптимизатора optimizer, который делает шаг в направлении, определяемом вычисленными градиентами.
- **loss.item()** — извлекаем численное значение потерь для текущего пакета, так как loss — это тензор, и .item() позволяет получить его скалярное значение.
- **epoch_loss += loss.item()** — добавляем это значение к epoch_loss, чтобы в конце эпохи получить общие потери по всем пакетам.
- **epoch_loss /= len(dataloader)** — делим накопленные потери за все пакеты на количество пакетов (len(dataloader)), чтобы получить средние потери за эпоху. Это значение является показателем того, насколько модель успешно обучилась на текущей эпохе.
- **print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}')** — выводим номер текущей эпохи и средние потери, округленные до 4 знаков после запятой, для отслеживания процесса обучения.
- 1. **if epoch_loss < best_loss:** — проверяем, улучшились ли потери. Если средние потери за текущую эпоху (epoch_loss) меньше, чем значение best_loss:
      - **best_loss = epoch_loss** — обновляем best_loss, так как модель достигла новых минимальных потерь.
      - **counter = 0** — сбрасываем счетчик на 0, так как улучшение потерь обнаружено.
      - **torch.save(model.state_dict(), "model.pth")** — сохраняем текущие параметры модели (веса) в файл "model.pth". Это гарантирует, что при завершении обучения у нас останется версия модели с минимальными потерями.

- 2. Если потери не улучшились (ветка else):
      - **counter += 1** — увеличиваем счетчик на 1, так как потери не уменьшились.
      - **if counter >= patience:** — проверяем, достиг ли счетчик значения patience. Если потери не улучшались в течение последних 10 эпох (patience):
      - **print("Раннее завершение обучения")** — выводим сообщение о завершении обучения из-за отсутствия улучшения.
      - **break** — прерываем цикл обучения, так как модель достигла предела терпения без улучшения потерь.
### Оценка точности и предсказание
```python
def calculate_accuracy(model, X_tensor, y_tensor, threshold=0.4):
    correct_predictions = 0
    total_predictions = y_tensor.shape[0]
    with torch.no_grad():
        predictions = model(X_tensor)
        predicted_labels = (predictions > threshold).int()
        for i in range(total_predictions):
            if torch.equal(predicted_labels[i], y_tensor[i].int()):
                correct_predictions += 1
    return correct_predictions / total_predictions * 100
```
Порог точности — определяет точность модели, сравнивая предсказания с истинными метками.
- **def calculate_accuracy(...)** — определяем функцию calculate_accuracy, которая будет вычислять точность предсказаний модели.
- Аргументы функции:
  - **model** — обученная модель, чьи предсказания мы хотим оценить.
  - **X_tensor** — тензор с признаками для всех текстов. Это входные данные, на которых будет проверяться точность модели.
  - **y_tensor** — тензор с истинными метками для каждого примера. Эти значения мы будем сравнивать с предсказанными.
  - **threshold=0.4** — порог, определяющий, при какой вероятности модель будет считать класс "активным" (то есть равным 1). Порог по умолчанию установлен на 0.4, но его можно изменить в зависимости от задачи.
- **correct_predictions = 0** — инициализируем переменную correct_predictions для подсчета количества правильных предсказаний. Каждый раз, когда предсказание модели совпадает с истинной меткой, мы увеличиваем этот счетчик на 1.
- **total_predictions = y_tensor.shape[0]** — сохраняем общее количество примеров в переменной total_predictions.
- **y_tensor.shape[0]** возвращает число строк в y_tensor, что равно количеству примеров. Это значение необходимо для расчета общей точности.
- **with torch.no_grad():** — блок torch.no_grad() отключает автоматическое вычисление градиентов, что экономит память и ускоряет вычисления. Поскольку мы только оцениваем модель, градиенты здесь не нужны.
- **predictions = model(X_tensor)** — пропускаем все входные данные X_tensor через модель, чтобы получить предсказания.
  - predictions будет содержать вероятности для каждого класса для каждого примера
- **predicted_labels = (predictions > threshold).int()** — переводим вероятности предсказаний в бинарные значения (0 или 1) на основе порога threshold.
  - **(predictions > threshold)** возвращает True для тех значений, которые выше порога, и False для значений ниже порога.
  - **.int()** преобразует True и False в 1 и 0 соответственно, что позволяет сравнить эти значения с бинарными метками в y_tensor.
- **for i in range(total_predictions):** — проходимся по каждому примеру, чтобы сравнить предсказанные метки с истинными.
- **torch.equal(predicted_labels[i], y_tensor[i].int())** — сравниваем предсказанные метки для примера i (predicted_labels[i]) с истинными метками (y_tensor[i]).
  - **torch.equal(...)** возвращает True, если все значения в predicted_labels[i] совпадают с y_tensor[i].
  - **y_tensor[i].int()** — применяем .int() к y_tensor[i], чтобы быть уверенными, что и предсказанные, и истинные метки находятся в одном формате.
- **correct_predictions += 1** — если предсказание совпадает с истинной меткой, увеличиваем счетчик correct_predictions на 1.
- **correct_predictions / total_predictions** — делим количество правильных предсказаний на общее количество примеров, чтобы получить долю правильных предсказаний.
- * **100**— умножаем на 100, чтобы выразить точность в процентах.
- **return ...** — возвращаем значение точности как итог работы функции.

  Функция calculate_accuracy подсчитывает процент примеров, в которых предсказанные метки совпадают с истинными метками для всех классов. Порог threshold позволяет гибко регулировать, при каком значении вероятности модель считает класс "активным" (1). Этот механизм полезен, например, для мультиклассовых задач, где можно управлять чувствительностью и специфичностью модели, изменяя порог.
